{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import numpy as np\n",
    "from beakerx import *\n",
    "from beakerx.object import beakerx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sci\n",
    "from copy import copy\n",
    "from IPython.display import HTML\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Code/')\n",
    "from utils import *\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In most stock market, there is a long tail of very small firms. It is well known that even though small firms comprise a large share of total companies, they take up a vanishingly small share of the market cap. This fact is highly problematic for implementing factor portfolios that construct anomaly deciles by equalizing *counts* of companies within anomaly deciles. This notebook explores the implication of constructing the momentum portfolio by equalizing the market cap within each decile. In doing so I create a better approximation to a momentum portfolio that can be implemented at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pd.read_hdf('../Output/merged.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate what the percentiles by market cap are when just weighted by market cap itself\n",
    "januaries = stocks.loc[(stocks.index.get_level_values(1).month == 1)]\n",
    "break_quantiles = [0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the CRSP-Compustat merged database. Below I show the top 10 companies by market cap as of January 2018 to give transparency into the data I'm using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b9979dd0f343d486e3c4e342a09bb5",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "januaries.xs('2018-01-31', level = 'datadate', drop_level = False).sort_values(['Market Cap (Billions, CRSP)'], ascending = False).loc[:, ['Company Name', 'Market Cap (Billions, CRSP)']].iloc[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distribution of Firm Size\n",
    "\n",
    "I first plot the distribution of market caps for the NYSE sample, weighting all companies equally. The median market cap is around $3 billion dollars as of January 2018. This is not tiny but still very small! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1a073689814f31b96817d95b44e181",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def all_quantiles(quantiles, plot_title, **kwargs):\n",
    "    plot = TimePlot(title = plot_title, legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.TOP),\\\n",
    "                        initWidth = 1000, **kwargs)\n",
    "    \n",
    "    for c in quantiles.columns:\n",
    "        plot.add(Line(displayName = c, \\\n",
    "                      x = quantiles.index.get_level_values(0),\\\n",
    "                      y = quantiles[c]))\n",
    "    return plot\n",
    "raw_quantiles = januaries.loc[januaries['Exchange Code'] == 1, 'Market Cap (Billions, CRSP)'].groupby(by = ['datadate']).quantile(q = break_quantiles).unstack()\n",
    "all_quantiles(raw_quantiles, 'Distribution of Firm Size (Unweighted)', logY = True, logYBase = np.exp(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I compute the weighted quantiles, using market caps as weights. Thus the 50'th percentile weighted market cap means that half of all market capitalization is held by firms smaller than that size. These weighted quantiles are much higher. Roughly 5% of total market cap is contained in firms with market cap less than $3.5 billion dollars. Thus the total market cap of the firms that make up the bottom 50\\% of the distribution of firm size is less than 5\\% of total market capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834a78c0f91b492bbbd360c03c41ae3f",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def weighted_quantiles_to_dataframe(x, quantiles, weights = None):\n",
    "    array = weighted_quantile(x, quantiles, weights)\n",
    "    df_dict = {}\n",
    "    for tup in zip(quantiles, array):\n",
    "        df_dict[tup[0]] = [tup[1]]\n",
    "    ret = pd.DataFrame.from_dict(df_dict)\n",
    "    ret.index = [x.name]\n",
    "    return ret\n",
    "\n",
    "weighted_market_cap_quantiles = januaries.loc[januaries['Exchange Code'] == 1, 'Market Cap (Billions, CRSP)'].groupby(by = ['datadate']).apply(lambda x: weighted_quantiles_to_dataframe(x, break_quantiles, x))\n",
    "all_quantiles(weighted_market_cap_quantiles, 'Distribution of Firm Size (Weighted)', logY = True, logYBase = np.exp(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilding anomaly portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks['Lagged Market Cap'] = stocks['Market Cap (Billions, CRSP)'].groupby(['Permco']).shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Permco'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-499747e9a93c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cumulative Return at t - 13'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cumulative Return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Permco'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cumulative Return at t - 3'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cumulative Return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Permco'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Return Momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cumulative Return at t - 3'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cumulative Return at t - 13'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Permco'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'datadate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compustat-crsp/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   6663\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   6664\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6665\u001b[0;31m                        observed=observed, **kwargs)\n\u001b[0m\u001b[1;32m   6666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m~/anaconda3/envs/compustat-crsp/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compustat-crsp/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/compustat-crsp/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m   3289\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Permco'"
     ]
    }
   ],
   "source": [
    "stocks['Cumulative Return at t - 13'] = stocks['Cumulative Return'].groupby(['Permco']).shift(13)\n",
    "stocks['Cumulative Return at t - 3'] = stocks['Cumulative Return'].groupby(['Permco']).shift(3)\n",
    "stocks['Return Momentum'] = stocks['Cumulative Return at t - 3'] - stocks['Cumulative Return at t - 13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.safe_index(['Permco', 'datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfabba08d6f4f0c99f8e41b98153871",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simple_plot(dataframe, variable, plot_title, **kwargs):\n",
    "    \"\"\"\n",
    "    Makes a simple line plot of \"variable\" from dataframe.\n",
    "    \n",
    "    :param dataframe -- a dataframe with a multi-index, the second level of which is the date variable\n",
    "    :param variable -- the variable to plot\n",
    "    :param plot_title -- the title to use for the plot\n",
    "    \"\"\"\n",
    "    plot = TimePlot(title = plot_title, legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.TOP),\\\n",
    "                        initWidth = 500, **kwargs)\n",
    "    plot.add(Line(displayName = variable, \\\n",
    "                  x = dataframe.index.get_level_values('datadate'),\\\n",
    "                  y = dataframe[variable]))\n",
    "    return plot\n",
    "\n",
    "apple = stocks.xs(7, level = 'Permco', drop_level = False)\n",
    "simple_plot(apple, 'Cumulative Return', 'Apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f236149185df4150ac48b65f74194b1f",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_plot(apple, 'Return Momentum', 'Momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8167d0b9df324899aa0414e986b421ea",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_in_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_positive_and_negative(array_like):\n",
    "    \"\"\"\n",
    "    Function that rescales all positive elements so that they add up to 1 and all negative elements so they add to negative one\n",
    "    \"\"\"\n",
    "    x = np.array(array_like, dtype = np.float64)\n",
    "    positive = np.where(x > 0)\n",
    "    negative = np.where(x < 0)\n",
    "    \n",
    "    pos_sum = np.sum(x[positive])\n",
    "    neg_sum = np.sum(x[negative]) * -1\n",
    "        \n",
    "    x[positive] = np.divide(x[positive], float(pos_sum))\n",
    "    x[negative] = np.divide(x[negative], float(neg_sum))\n",
    "    return x\n",
    "\n",
    "def build_portfolio_point_in_time(df_in_time, anomaly_variable, min_stock_count = 100):\n",
    "    types = ['Unweighted Sort', 'Weighted Sort', 'Linear Factor']\n",
    "    diag_vars = ['Company Name', 'Permco', 'Lagged Market Cap', 'Return', 'Sorting Variable']\n",
    "    # if_vars = [w + ' Inclusion Factor' for w in types]\n",
    "    weight_vars = [w + ' Weight' for w in types]\n",
    "    contrib_vars = [w + ' Contribution' for w in types]\n",
    "    all_cols = diag_vars + weight_vars + contrib_vars # + if_vars\n",
    "    \n",
    "    df_in_time = df_in_time.loc[~pd.isnull(df_in_time[anomaly_variable])]\n",
    "    df_in_time = df_in_time.loc[~pd.isnull(df_in_time['Lagged Market Cap'])]\n",
    "    \n",
    "    if df_in_time.shape[0] < min_stock_count:\n",
    "        ret = pd.DataFrame(index = [df_in_time.index[0]], columns = all_cols)\n",
    "        ret.index.name = 'datadate'\n",
    "        return ret\n",
    "    \n",
    "    # First trim the anomaly variable\n",
    "    anomaly_cuts = weighted_quantile(df_in_time[anomaly_variable], [0.05, 0.5, 0.95], df_in_time['Lagged Market Cap'])\n",
    "    df_in_time['Sorting Variable'] = df_in_time[anomaly_variable] - anomaly_cuts[1]\n",
    "    anomaly_cuts = anomaly_cuts - anomaly_cuts[1]\n",
    "    \n",
    "    # Now do all the sorts\n",
    "    unweighted_sort_breakpoints = weighted_quantile(df_in_time.loc[df_in_time['Exchange Code'] == 1, 'Sorting Variable'], [0, 0.33, 0.67, 1]) # Use NYSE breakpoints\n",
    "    weighted_sort_breakpoints = weighted_quantile(df_in_time['Sorting Variable'], [0, 0.33, 0.67, 1], sample_weight = df_in_time['Lagged Market Cap'])\n",
    "    \n",
    "    df_in_time['Unweighted Sort Inclusion Factor'] = pd.cut(df_in_time['Sorting Variable'], unweighted_sort_breakpoints, labels = False) - 1\n",
    "    df_in_time['Weighted Sort Inclusion Factor'] = pd.cut(df_in_time['Sorting Variable'], weighted_sort_breakpoints, labels = False) - 1\n",
    "    df_in_time['Linear Factor Inclusion Factor'] = winsorize_at_explicit_input(df_in_time['Sorting Variable'], anomaly_cuts[0], anomaly_cuts[2])\n",
    "    \n",
    "    for weight_type in types:\n",
    "        df_in_time['Unnormalized ' + weight_type + ' Portfolio Weights'] = df_in_time[weight_type + ' Inclusion Factor'] * df_in_time['Lagged Market Cap']\n",
    "        df_in_time[weight_type + ' Weight'] = normalize_positive_and_negative(df_in_time['Unnormalized ' + weight_type + ' Portfolio Weights'])\n",
    "        df_in_time[weight_type + ' Contribution'] = df_in_time[weight_type + ' Weight'] * df_in_time['Return']\n",
    "    \n",
    "    ret = df_in_time[all_cols]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_portfolio_through_time(crsp_data, anomaly_variable):\n",
    "    through_time_portfolio = crsp_data.groupby(by = ['datadate']).apply(build_portfolio_point_in_time, anomaly_variable = anomaly_variable)\n",
    "    return through_time_portfolio\n",
    "\n",
    "def calc_return_series(portfolio, ret_vars = ['Unweighted Sort Contribution', 'Weighted Sort Contribution', 'Linear Factor Contribution']):\n",
    "    return portfolio.loc[:, ret_vars].groupby(['datadate']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.safe_index(['datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = build_portfolio_through_time(stocks, 'Return Momentum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Fama French Momentum Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_series = calc_return_series(momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_returns = pd.melt(return_series.reset_index(), id_vars = ['datadate'], var_name = 'Portfolio Type', value_name = 'Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_returns['Log Return'] = np.log(long_returns['Return'] + 1)\n",
    "long_returns = long_returns.safe_index(['Portfolio Type'])\n",
    "long_returns['Cumulative Return'] = long_returns['Log Return'].groupby(by = ['Portfolio Type']).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7fb931804e34fcb80d41cd8c5e7a719",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample_returns = long_returns.reset_index().pivot(index = 'datadate', columns = 'Portfolio Type', values = 'Cumulative Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95862473ade443d9fe10859c5513f27",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_sample_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4ee239ce924decb34178484339676e",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def comparison_plot(dataframe, **kwargs):\n",
    "    \"\"\"\n",
    "    Makes a simple line plot of \"variable\" from dataframe.\n",
    "    \n",
    "    :param dataframe -- a dataframe with a multi-index, the second level of which is the date variable\n",
    "    :param variable -- the variable to plot\n",
    "    :param plot_title -- the title to use for the plot\n",
    "    \"\"\"\n",
    "    plot = TimePlot(title = 'Comparison of Portfolios', legendLayout=LegendLayout.HORIZONTAL,\\\n",
    "                          legendPosition=LegendPosition(position=LegendPosition.Position.TOP),\\\n",
    "                        initWidth = 1000, **kwargs)\n",
    "    \n",
    "    for c in dataframe.columns:\n",
    "        plot.add(Line(displayName = c, \\\n",
    "                      x = dataframe.index.get_level_values('datadate'),\\\n",
    "                      y = dataframe[c]))\n",
    "    return plot\n",
    "\n",
    "comparison_plot(small_sample_returns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
